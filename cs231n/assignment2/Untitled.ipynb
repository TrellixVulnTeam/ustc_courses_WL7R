{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function batch_normalization in module tensorflow.python.layers.normalization:\n",
      "\n",
      "batch_normalization(inputs, axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer=<tensorflow.python.ops.init_ops.Zeros object at 0x0000028EA8EADB38>, gamma_initializer=<tensorflow.python.ops.init_ops.Ones object at 0x0000028EA8EADBA8>, moving_mean_initializer=<tensorflow.python.ops.init_ops.Zeros object at 0x0000028EA8EADBE0>, moving_variance_initializer=<tensorflow.python.ops.init_ops.Ones object at 0x0000028EA8EADC18>, beta_regularizer=None, gamma_regularizer=None, training=False, trainable=True, name=None, reuse=None, renorm=False, renorm_clipping=None, renorm_momentum=0.99, fused=False)\n",
      "    Functional interface for the batch normalization layer.\n",
      "    \n",
      "    Reference: http://arxiv.org/abs/1502.03167\n",
      "    \n",
      "    \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
      "    Internal Covariate Shift\"\n",
      "    \n",
      "    Sergey Ioffe, Christian Szegedy\n",
      "    \n",
      "    Note: when training, the moving_mean and moving_variance need to be updated.\n",
      "    By default the update ops are placed in `tf.GraphKeys.UPDATE_OPS`, so they\n",
      "    need to be added as a dependency to the `train_op`. For example:\n",
      "    \n",
      "    ```python\n",
      "      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
      "      with tf.control_dependencies(update_ops):\n",
      "        train_op = optimizer.minimize(loss)\n",
      "    ```\n",
      "    \n",
      "    Arguments:\n",
      "      inputs: Tensor input.\n",
      "      axis: Integer, the axis that should be normalized (typically the features\n",
      "        axis). For instance, after a `Convolution2D` layer with\n",
      "        `data_format=\"channels_first\"`, set `axis=1` in `BatchNormalization`.\n",
      "      momentum: Momentum for the moving average.\n",
      "      epsilon: Small float added to variance to avoid dividing by zero.\n",
      "      center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n",
      "        is ignored.\n",
      "      scale: If True, multiply by `gamma`. If False, `gamma` is\n",
      "        not used. When the next layer is linear (also e.g. `nn.relu`), this can be\n",
      "        disabled since the scaling can be done by the next layer.\n",
      "      beta_initializer: Initializer for the beta weight.\n",
      "      gamma_initializer: Initializer for the gamma weight.\n",
      "      moving_mean_initializer: Initializer for the moving mean.\n",
      "      moving_variance_initializer: Initializer for the moving variance.\n",
      "      beta_regularizer: Optional regularizer for the beta weight.\n",
      "      gamma_regularizer: Optional regularizer for the gamma weight.\n",
      "      training: Either a Python boolean, or a TensorFlow boolean scalar tensor\n",
      "        (e.g. a placeholder). Whether to return the output in training mode\n",
      "        (normalized with statistics of the current batch) or in inference mode\n",
      "        (normalized with moving statistics). **NOTE**: make sure to set this\n",
      "        parameter correctly, or else your training/inference will not work\n",
      "        properly.\n",
      "      trainable: Boolean, if `True` also add variables to the graph collection\n",
      "        `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n",
      "      name: String, the name of the layer.\n",
      "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
      "        by the same name.\n",
      "      renorm: Whether to use Batch Renormalization\n",
      "        (https://arxiv.org/abs/1702.03275). This adds extra variables during\n",
      "        training. The inference is the same for either value of this parameter.\n",
      "      renorm_clipping: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to\n",
      "        scalar `Tensors` used to clip the renorm correction. The correction\n",
      "        `(r, d)` is used as `corrected_value = normalized_value * r + d`, with\n",
      "        `r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,\n",
      "        dmax are set to inf, 0, inf, respectively.\n",
      "      renorm_momentum: Momentum used to update the moving means and standard\n",
      "        deviations with renorm. Unlike `momentum`, this affects training\n",
      "        and should be neither too small (which would add noise) nor too large\n",
      "        (which would give stale estimates). Note that `momentum` is still applied\n",
      "        to get the means and variances for inference.\n",
      "      fused: if `True`, use a faster, fused implementation based on\n",
      "        nn.fused_batch_norm. If `None`, use the fused implementation if possible.\n",
      "    \n",
      "    Returns:\n",
      "      Output tensor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.layers.batch_normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
